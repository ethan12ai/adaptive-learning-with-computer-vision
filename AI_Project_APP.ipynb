{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken # Your Authtoken Here"
      ],
      "metadata": {
        "id": "wfSXLF7psCOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTU0qP8rr5Eg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "class HiddenPrints:\n",
        "    def __enter__(self):\n",
        "        self._original_stdout = sys.stdout\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "\n",
        "with HiddenPrints():\n",
        "    # Prepare data\n",
        "    DATA_ROOT = '/content/data'\n",
        "    os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "\n",
        "    !pip -q install streamlit\n",
        "    !pip -q install pyngrok\n",
        "    # projet-specific installs\n",
        "    !pip install deepface\n",
        "    !pip install tf_keras\n",
        "    !pip install opencv-python\n",
        "    !pip install dlib\n",
        "    !git clone https://github.com/antoinelame/GazeTracking.git\n",
        "\n",
        "    from pyngrok import ngrok\n",
        "    import streamlit\n",
        "def launch_website():\n",
        "    # Function to launch the Streamlit web app using ngrok for tunneling\n",
        "    print(\"Click this link to try your web app:\")\n",
        "    if ngrok.get_tunnels():\n",
        "        ngrok.kill()  # Kill existing ngrok tunnels\n",
        "    tunnel = ngrok.connect()  # Open a ngrok tunnel to the streamlit app\n",
        "    print(tunnel.public_url)  # Print only the public URL of the ngrok tunnel\n",
        "    !streamlit run --server.port 80 app.py >/dev/null  # Launch the Streamlit app"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Haar cascade .xml file for face detection\n",
        "!wget https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml"
      ],
      "metadata": {
        "id": "8_smaWPFr-zK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.py\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from deepface import DeepFace\n",
        "from google.colab.patches import cv2_imshow\n",
        "from GazeTracking.gaze_tracking import GazeTracking\n",
        "import imageio.v3 as iio\n",
        "\n",
        "\n",
        "# Load Haar cascade classifier for face detection\n",
        "face_cascade = cv2.CascadeClassifier('./haarcascade_frontalface_default.xml')\n",
        "gaze = GazeTracking()\n",
        "\n",
        "\n",
        "# Emotion detection model\n",
        "\n",
        "def detect_emotion (frame, emotion_count):\n",
        "  # Convert frame to grayscale\n",
        "  gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # Convert grayscale to RGB\n",
        "  rgb_frame = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2RGB)\n",
        "  # Detect face within frames\n",
        "  faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.02, minNeighbors=1, minSize=(200,200))\n",
        "\n",
        "  if len(faces) > 0:\n",
        "    (x, y, w, h) = faces[0]\n",
        "    # Find face ROI (Region of Interest)\n",
        "    face_roi = rgb_frame[y:y + h, x:x + w]\n",
        "\n",
        "    # Perform emotion detection on face ROI\n",
        "    result = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
        "\n",
        "    # Determine dominant emotion in frame\n",
        "    emotion = result[0]['dominant_emotion']\n",
        "\n",
        "    # Track dominant emotions in frames\n",
        "    if emotion in emotion_count.keys():\n",
        "      emotion_count[emotion] += 1\n",
        "    else:\n",
        "      emotion_count[emotion] = 1\n",
        "\n",
        "    # Outline and label face with predicted emotion\n",
        "    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
        "    cv2.putText(frame, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
        "\n",
        "\n",
        "# Gaze detection model\n",
        "\n",
        "def detect_gaze (gaze, gaze_count):\n",
        "  # Return frame with pupils highlighted\n",
        "  frame = gaze.annotated_frame()\n",
        "  text = \"\"\n",
        "\n",
        "  # Track dominant gaze direction\n",
        "  if gaze.is_right():\n",
        "    text = \"Looking right\"\n",
        "    gaze_count[\"right\"] += 1\n",
        "  elif gaze.is_left():\n",
        "    text = \"Looking left\"\n",
        "    gaze_count[\"left\"] += 1\n",
        "  elif gaze.is_center():\n",
        "    text = \"Looking center\"\n",
        "    gaze_count[\"center\"] += 1\n",
        "\n",
        "  # Label face with predicted gaze direction\n",
        "  cv2.putText(frame, text, (60, 60), cv2.FONT_HERSHEY_DUPLEX, 2, (255, 0, 0), 2)\n",
        "  cv2_imshow(frame)\n",
        "\n",
        "\n",
        "# Logic fuction to determine focus level\n",
        "\n",
        "def assess_focus(emotion_count, gaze_count, verbose=False):\n",
        "  sorted_emotions = sorted(emotion_count, key = emotion_count.get, reverse=True)\n",
        "\n",
        "  # Identify dominant emotion\n",
        "  dominant_emotion = sorted_emotions[0]\n",
        "  second_index = 1 if sorted(sorted_emotions[:2]) != sorted(['happy', 'neutral']) else 2\n",
        "  next_to_dominant_emotion = sorted_emotions[second_index]\n",
        "\n",
        "  # Get top score and second top score\n",
        "  max_score = emotion_count[dominant_emotion]\n",
        "  next_to_max_score = emotion_count[next_to_dominant_emotion]\n",
        "\n",
        "  # Compare the two top scores to assess dominant emotion\n",
        "  has_dominant_emotion = max_score > 1.1 * next_to_max_score\n",
        "\n",
        "  # Assess emotion and gaze focus\n",
        "  emotion_focused = has_dominant_emotion and dominant_emotion in ['happy', 'neutral']\n",
        "  gaze_focused = gaze_count[\"center\"] > gaze_count[\"left\"] and gaze_count[\"center\"] > gaze_count[\"right\"]\n",
        "\n",
        "  # Compare emotion and gaze focus to assess overall focus\n",
        "  if emotion_focused and gaze_focused:\n",
        "    verdict = \"Nice focus! Let's move to the next level.\"\n",
        "  elif emotion_focused or gaze_focused:\n",
        "    verdict = \"Let's continue with this level.\"\n",
        "  else:\n",
        "    verdict = \"You seem to be distracted. Let's move down a level.\"\n",
        "\n",
        "  if verbose:\n",
        "    verdict += f\"\\n\\nEmotion counts: {emotion_count}\"\n",
        "    verdict += f\"\\n\\nGaze counts: {gaze_count}\"\n",
        "\n",
        "  return verdict\n",
        "\n",
        "\n",
        "\n",
        "def categorize_video(video_filename):\n",
        "  vid = cv2.VideoCapture(video_filename)\n",
        "\n",
        "  emotion_count = {}\n",
        "  gaze_count = {\"center\":0, \"left\":0, \"right\":0}\n",
        "\n",
        "  frame_n = 0\n",
        "\n",
        "  while True:\n",
        "    # Capture video frame by frame\n",
        "    ret, frame = vid.read()\n",
        "    if frame is None:\n",
        "      break\n",
        "\n",
        "    # Read every 30 frames\n",
        "    if frame_n%30==0:\n",
        "      gaze.refresh(frame)\n",
        "\n",
        "      # Run emotion detection\n",
        "      detect_emotion(frame, emotion_count)\n",
        "\n",
        "      # Run gaze detection\n",
        "      detect_gaze(gaze, gaze_count)\n",
        "    print(emotion_count)\n",
        "    frame_n += 1\n",
        "\n",
        "  # Release capture and close all windows\n",
        "  vid.release()\n",
        "  cv2.destroyAllWindows()\n",
        "\n",
        "  # print(\"Here is your emotion analysis: \" + str(emotion_count) + \"\\n\")\n",
        "  # print(\"Here is your gaze analysis: \" + str(gaze_count) + \"\\n\")\n",
        "\n",
        "  # Run logic function to assess focus\n",
        "  return assess_focus(emotion_count, gaze_count, verbose=True)"
      ],
      "metadata": {
        "id": "YetkUs-YsDa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from utils import categorize_video\n",
        "from PIL import Image\n",
        "\n",
        "st.title('Adaptive Learning Through Emotion Detection and Gaze Tracking')\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Import Camera Video Here\", type=[\"mov\", \"mp4\"])\n",
        "\n",
        "if uploaded_file is not None: # Runs only when user uploads video\n",
        "    vid = uploaded_file.name\n",
        "    with open(vid, mode='wb') as f:\n",
        "        f.write(uploaded_file.read()) # Save video to disk\n",
        "\n",
        "    st.video(data=uploaded_file, format=uploaded_file.type, start_time=0, subtitles=None, end_time=None, loop=False, autoplay=False, muted=False)\n",
        "    response = categorize_video(vid)\n",
        "    with st.chat_message(\"Your virtual teacher\", avatar=\"ðŸ¤–\"):\n",
        "        st.write(response)"
      ],
      "metadata": {
        "id": "tUK5t1Y-sLLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "launch_website()"
      ],
      "metadata": {
        "id": "sYqluDCosND8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}