{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96-v4t2NIW5G"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MmeW61mIcwE"
      },
      "outputs": [],
      "source": [
        "!ls '/content/drive/MyDrive/AI Research Project/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kS_RtTWdIeZc"
      },
      "outputs": [],
      "source": [
        "# Installing required dependencies\n",
        "!pip install deepface\n",
        "!pip install tf_keras\n",
        "!pip install opencv-python\n",
        "!pip install dlib\n",
        "!git clone https://github.com/antoinelame/GazeTracking.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-x8IIefqIgiY"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from deepface import DeepFace\n",
        "from google.colab.patches import cv2_imshow\n",
        "from GazeTracking.gaze_tracking import GazeTracking\n",
        "\n",
        "# Load Haar cascade classifier for face detection\n",
        "face_cascade = cv2.CascadeClassifier('/content/drive/MyDrive/AI Research Project/haarcascade_frontalface_default.xml')\n",
        "gaze = GazeTracking()\n",
        "\n",
        "\n",
        "# Emotion detection model\n",
        "\n",
        "def detect_emotion (emotion_count):\n",
        "  # Convert frame to grayscale\n",
        "  gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # Convert grayscale to RGB\n",
        "  rgb_frame = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "  # Detect face within frames\n",
        "  faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.02, minNeighbors=1, minSize=(200,200))\n",
        "\n",
        "  if len(faces) > 0:\n",
        "    (x, y, w, h) = faces[0]\n",
        "    # Find face ROI (Region of Interest)\n",
        "    face_roi = rgb_frame[y:y + h, x:x + w]\n",
        "\n",
        "    # Perform emotion detection on face ROI\n",
        "    result = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
        "\n",
        "    # Determine dominant emotion in frame\n",
        "    emotion = result[0]['dominant_emotion']\n",
        "\n",
        "    # Track dominant emotions in frames\n",
        "    if emotion in emotion_count.keys():\n",
        "      emotion_count[emotion] += 1\n",
        "    else:\n",
        "      emotion_count[emotion] = 1\n",
        "\n",
        "    # Outline and label face with predicted emotion\n",
        "    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
        "    cv2.putText(frame, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
        "\n",
        "\n",
        "# Gaze detection model\n",
        "\n",
        "def detect_gaze (gaze_count):\n",
        "  # Return frame with pupils highlighted\n",
        "  frame = gaze.annotated_frame()\n",
        "  text = \"\"\n",
        "\n",
        "  # Track dominant gaze direction\n",
        "  if gaze.is_right():\n",
        "    text = \"Looking right\"\n",
        "    gaze_count[\"right\"] += 1\n",
        "  elif gaze.is_left():\n",
        "    text = \"Looking left\"\n",
        "    gaze_count[\"left\"] += 1\n",
        "  elif gaze.is_center():\n",
        "    text = \"Looking center\"\n",
        "    gaze_count[\"center\"] += 1\n",
        "\n",
        "  # Label face with predicted gaze direction\n",
        "  cv2.putText(frame, text, (60, 60), cv2.FONT_HERSHEY_DUPLEX, 2, (255, 0, 0), 2)\n",
        "  cv2_imshow(frame)\n",
        "\n",
        "\n",
        "# Logic fuction to determine focus level\n",
        "\n",
        "def assess_focus(emotion_count, gaze_count):\n",
        "  sorted_emotions = sorted(emotion_count, key = emotion_count.get, reverse=True)\n",
        "\n",
        "  # Identify dominant emotion\n",
        "  dominant_emotion = sorted_emotions[0]\n",
        "  second_index = 1 if sorted(sorted_emotions[:2]) != sorted(['happy', 'neutral']) else 2\n",
        "  next_to_dominant_emotion = sorted_emotions[second_index]\n",
        "\n",
        "  # Get top score and second top score\n",
        "  max_score = emotion_count[dominant_emotion]\n",
        "  next_to_max_score = emotion_count[next_to_dominant_emotion]\n",
        "\n",
        "  # Compare the two top scores to assess dominant emotion\n",
        "  has_dominant_emotion = max_score > 1.1 * next_to_max_score\n",
        "\n",
        "  # Assess emotion and gaze focus\n",
        "  emotion_focused = has_dominant_emotion and dominant_emotion in ['happy', 'neutral']\n",
        "  gaze_focused = gaze_count[\"center\"] > gaze_count[\"left\"] and gaze_count[\"center\"] > gaze_count[\"right\"]\n",
        "\n",
        "  # Compare emotion and gaze focus to assess overall focus\n",
        "  if emotion_focused and gaze_focused:\n",
        "    print(\"Nice focus! Let's move to the next level.\")\n",
        "  elif emotion_focused or gaze_focused:\n",
        "    print(\"Let's continue with this level.\")\n",
        "  else:\n",
        "    print(\"You seem to be distracted. Let's move down a level.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "4WQJfz9LKS2P"
      },
      "outputs": [],
      "source": [
        "vid = cv2.VideoCapture('/content/drive/MyDrive/AI Research Project/Datasets/Dataset 3 (Daytime)/happy_short.mov')\n",
        "\n",
        "emotion_count = {}\n",
        "gaze_count = {\"center\":0, \"left\":0, \"right\":0}\n",
        "\n",
        "frame_n = 0\n",
        "\n",
        "while True:\n",
        "  # Capture video frame by frame\n",
        "  ret, frame = vid.read()\n",
        "  if frame is None:\n",
        "    break\n",
        "\n",
        "  # Read every 30 frames\n",
        "  if frame_n%30==0:\n",
        "    gaze.refresh(frame)\n",
        "\n",
        "    # Run emotion detection\n",
        "    detect_emotion(emotion_count)\n",
        "\n",
        "    # Run gaze detection\n",
        "    detect_gaze(gaze_count)\n",
        "\n",
        "  frame_n += 1\n",
        "\n",
        "# Release capture and close all windows\n",
        "vid.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "print(\"Here is your emotion analysis: \" + str(emotion_count) + \"\\n\")\n",
        "print(\"Here is your gaze analysis: \" + str(gaze_count) + \"\\n\")\n",
        "\n",
        "# Run logic function to assess focus\n",
        "assess_focus(emotion_count, gaze_count)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}